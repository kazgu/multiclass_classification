{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn 用 SVM,RandomForest,XGBoost 多分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "这我解决的问题是用SVM，XGBoost，RandomForest来对用户评论进行情感分析，训练集是2万，验证集2千，测试集2千，标注的类型总4个类，正面情感，\n",
    "负面情感，中立情感，没提到 分别是 1，-1，0，-2 来标注，接下来我分别创建3个分类模型，对此问题进行分析。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.sklearn SVM.SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM 主要思想是在样本空间中建立一个最优超平面，将两类样本的隔离边缘最大化，是结构风险最小化（ SRM ）的近似实现\n",
    "单一的svm是只能二分类，但我们要分类的是多分类，所以用svm.svc multiclass 多分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.惩罚系数C\n",
    "即为我们第二节中SVM分类模型原型形式和对偶形式中的惩罚系数C，默认为1，一般需要通过交叉验证来选择一个合适的C。\n",
    "一般来说，如果噪音点较多时，C需要小一些。\n",
    "C类似于正则化中1λ的作用。C越大，拟合非线性的能力越强\n",
    "2.核函数 kernel\n",
    "核函数能提高模型的Feature维度（低维到高维），从而使SVM具有较好的非线性拟合能力。\n",
    "3.核函数参数degree\n",
    "核函数的系数(‘Poly’, ‘RBF’ and ‘Sigmoid’), 默认是gamma = 1 / n_features; \n",
    "\n",
    "使用SVM时，有两个点要注意：\n",
    "若使用核函数，一定要对Feature做Feature Scaling(Normalization)\n",
    "若训练集m太小，但Feature数量n很大，则训练数据不足以拟合复杂的非线性模型，这种情况下只能用linear-kernel（就是fi=xi\n",
    "）不能用高斯核\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score , roc_auc_score , roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "from sklearn.metrics import f1_score\n",
    "from DrawUtil import DrawGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentWord(cont): \n",
    "    c = [] \n",
    "    for i in cont:\n",
    "       a = list(jieba.cut(i))\n",
    "       b = \" \".join(a)\n",
    "       c.append(b) \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过以上函数对中文文本分词，用jieba来分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(d_train ,d_valid, d_test):\n",
    "    print(\"训练样本 = %d\" % len(d_train))\n",
    "    print(\"验证样本 = %d\" % len(d_valid))\n",
    "    print(\"测试样本 = %d\" %len(d_test))\n",
    "    content_train=segmentWord(d_train['content'])\n",
    "    content_valid=segmentWord(d_valid['content'])\n",
    "    content_test=segmentWord(d_test['content'])\n",
    "    \n",
    "    lb_train=d_train['label']\n",
    "    lb_valid=d_valid['label']\n",
    "    #vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df=2 ) #tf-idf特征抽取ngram_range=(1,2)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word',min_df=3,token_pattern=r\"(?u)\\b\\w\\w+\\b\")\n",
    "    features = vectorizer.fit_transform(content_train)\n",
    "    weight=features.toarray()\n",
    "    print(\"训练样本特征表长度为 \" + str(features.shape))\n",
    "    # print(vectorizer.get_feature_names()) #特征名展示\n",
    "    valid_features = vectorizer.transform(content_valid)\n",
    "    valid_weight=valid_features.toarray()\n",
    "    print(\"验证样本特征表长度为 \"+ str(valid_features.shape))\n",
    "    test_features = vectorizer.transform(content_test)\n",
    "    test_weight=test_features.toarray()\n",
    "    print(\"测试样本特征表长度为 \"+ str(test_features.shape))\n",
    "    data=[content_train,content_valid,content_test,lb_train,lb_valid,features,valid_features,test_features,weight,valid_weight,test_weight]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上函数是用来准备数据并向量化，提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(data):\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"SVM 开始训练了\")\n",
    "    content_train,content_valid,content_test,lb_train,lb_valid,features,valid_features,test_features,weight,valid_weight,test_weight=data\n",
    "    #支持向量机\n",
    "    #C: 目标函数的惩罚系数C，用来平衡分类间隔margin和错分样本的，default C = 1.0\n",
    "    svmmodel =OneVsRestClassifier(SVC(C=1,kernel= \"linear\",degree=3))#kernel：参数选择有rbf, linear, poly, Sigmoid, 默认的是\"RBF\";ß\n",
    "#     parameters = {\n",
    "#     \"estimator__C\": [1,2,4,8],\n",
    "#     \"estimator__kernel\": [\"poly\",\"rbf\"],\n",
    "#     \"estimator__degree\":[1, 2, 3, 4],\n",
    "#     }\n",
    "#     model_tunning = GridSearchCV(svmmodel, param_grid=parameters)\n",
    "\n",
    "    svmmodel.fit(features , lb_train)\n",
    "#     print( model_tunning.best_score_)\n",
    "#     print (model_tunning.best_params_)\n",
    "    preds=svmmodel.predict(valid_features)\n",
    "    f1=f1_score(lb_valid, preds, average='macro')\n",
    "    scores.append(f1)\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上SVM模型中，我试调C，kernel，degree 等参数，通过发现数据集比较少的时候linear的效果好点，数据集离散的用poly，还用GridSearchCV来参数优化了，进过实验发现，在数据少的情况下自动调优效果不明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林是一个集成工具，它使用观测数据的子集和变量的子集来建立一个决策树。\n",
    "它建立多个这样的决策树，然后将他们合并在一起以获得更准确和稳定的预测。 \n",
    "这样做最直接的事实是，在这一组独立的预测结果中，用投票方式得到一个最高投票结果，这个比单独使用最好模型预测的结果要好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.ensemble import RandomForestRegressor  \n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_model(data):\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"RandomForest 开始训练了\")\n",
    "    content_train,content_valid,content_test,lb_train,lb_valid,features,valid_features,test_features,weight,valid_weight,test_weight=data\n",
    "    rf=RandomForestRegressor()#这里使用了默认的参数设置 \n",
    "    print(rf)\n",
    "    rf.fit(features,lb_train)#进行模型的训练\n",
    "    preds=rf.predict(valid_features)\n",
    "    npre=[]\n",
    "    for lb in preds:\n",
    "        npre.append(np.round(lb))\n",
    "    f1=f1_score(lb_valid, npre, average='macro')\n",
    "    scores.append(f1)\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下分析一下一些参数：\n",
    "1.max_features：\n",
    "随机森林允许单个决策树使用特征的最大数量。\n",
    "    增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。\n",
    "然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。 \n",
    "但是，可以肯定，你通过增加max_features会降低算法的速度。 因此，你需要适当的平衡和选择最佳max_features。\n",
    "2.n_estimators 投票数：\n",
    "在利用最大投票数或平均值来预测之前，你想要建立子树的数量。 \n",
    "较多的子树可以让模型有更好的性能，但同时让你的代码变慢。 你应该选择尽可能高的值，只要你的处理器能够承受的住，因为这使你的预测更好更稳定。\n",
    "3.min_sample_leaf：\n",
    " 叶是决策树的末端节点。 较小的叶子使模型更容易捕捉训练数据中的噪声。\n",
    "一般来说，偏向于将最小叶子节点数目设置为大于50。在自己的情况中，应该尽量尝试多种叶子大小种类，以找到最优的那个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost是以决策树（CART）为基学习器的GB算法\n",
    "Gradient boosting(GB)\n",
    "机器学习中的学习算法的目标是为了优化或者说最小化loss Function， Gradient boosting的思想是迭代生多个（M个）弱的模型，\n",
    "然后将每个弱模型的预测结果相加，后面的模型Fm+1(x)基于前面学习模型的Fm(x)的效果生成的，关系如下：\n",
    "                                                \n",
    "                                                                Fm+1(x)=Fm(x)+h(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import csv\n",
    "import jieba\n",
    "# jieba.load_userdict('wordDict.txt') \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # 读取训练集 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_model(data):\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"XGBoost 开始训练了\")\n",
    "    content_train,content_valid,content_test,lb_train,lb_valid,features,valid_features,test_features,weight,valid_weight,test_weight=data\n",
    "\n",
    "    lb_train=lb_train.replace(-1,2) #替换标签\n",
    "    lb_train=lb_train.replace(-2,3)\n",
    "\n",
    "    lb_valid=lb_valid.replace(-1,2) #替换标签\n",
    "    lb_valid=lb_valid.replace(-2,3)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(weight, label=lb_train) \n",
    "    dvalid = xgb.DMatrix(valid_weight, label=lb_valid) \n",
    "    dtest = xgb.DMatrix(test_weight) # label可以不要，此处需要是为了测试效果\n",
    "    \n",
    "    param_test7 = { 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]}\n",
    "    gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4, min_child_weight=6, \n",
    "    gamma=0.1, subsample=0.8, colsample_bytree=0.8, objective= 'multi:softmax'),param_grid = param_test7, cv=5)\n",
    "    gsearch7.fit(weight,lb_train)\n",
    "    print(gsearch7.grid_scores_)\n",
    "    print(gsearch7.best_params_, )\n",
    "    print(gsearch7.best_score_)\n",
    "#     param = {'max_depth':8, 'eta':0.1, 'eval_metric':'merror', 'silent':1, 'objective':'multi:softmax', 'num_class':4} # 参数 \n",
    "#     evallist = [(dtrain,'train'), (dvalid,'test')] # 这步可以不要，用于测试效果 \n",
    "#     num_round = 10 # 循环次数\n",
    "#     bst = xgb.train(param, dtrain, num_round, evallist) \n",
    "#     preds = bst.predict(dvalid)\n",
    "#     f1=f1_score(lb_valid, preds, average='macro')\n",
    "#     scores.append(f1)\n",
    "#     print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost 参数\n",
    "在运行XGBoost程序之前，必须设置三种类型的参数：通用类型参数（general parameters）、booster参数和学习任务参数（task parameters）。\n",
    "一般类型参数general parameters –参数决定在提升的过程中用哪种booster，常见的booster有树模型和线性模型。\n",
    "Booster参数-该参数的设置依赖于我们选择哪一种booster模型。\n",
    "学习任务参数task parameters-参数的设置决定着哪一种学习场景，例如，回归任务会使用不同的参数来控制着排序任务。\n",
    "命令行参数-一般和xgboost的CL版本相关。\n",
    "Booster参数：\n",
    "1. eta[默认是0.3] 和GBM中的learning rate参数类似。通过减少每一步的权重，可以提高模型的鲁棒性。典型值0.01-0.2\n",
    "2. min_child_weight[默认是1] 决定最小叶子节点样本权重和。当它的值较大时，可以避免模型学习到局部的特殊样本。但如果这个值过高，会导致欠拟合。这个参数需要用cv来调整\n",
    "3. max_depth [默认是6] 树的最大深度，这个值也是用来避免过拟合的3-10\n",
    "4. max_leaf_nodes 树上最大的节点或叶子的数量，可以代替max_depth的作用，应为如果生成的是二叉树，一个深度为n的树最多生成2n个叶子,如果定义了这个参数max_depth会被忽略\n",
    "5. gamma[默认是0] 在节点分裂时，只有在分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。这个参数值越大，算法越保守。\n",
    "6. max_delta_step[默认是0] 这参数限制每颗树权重改变的最大步长。如果是0意味着没有约束。如果是正值那么这个算法会更保守，通常不需要设置。\n",
    "7. subsample[默认是1] 这个参数控制对于每棵树，随机采样的比例。减小这个参数的值算法会更加保守，避免过拟合。但是这个值设置的过小，它可能会导致欠拟合。典型值：0.5-1\n",
    "8. colsample_bytree[默认是1] 用来控制每颗树随机采样的列数的占比每一列是一个特征0.5-1\n",
    "9. colsample_bylevel[默认是1] 用来控制的每一级的每一次分裂，对列数的采样的占比。\n",
    "10. lambda[默认是1] 权重的L2正则化项\n",
    "11. alpha[默认是1] 权重的L1正则化项\n",
    "12. scale_pos_weight[默认是1] 各类样本十分不平衡时，把这个参数设置为一个正数，可以使算法更快收敛。\n",
    "通用参数：\n",
    "1． booster[默认是gbtree]\n",
    "选择每次迭代的模型，有两种选择：gbtree基于树的模型、gbliner线性模型\n",
    "2． silent[默认是0]\n",
    "当这个参数值为1的时候，静默模式开启，不会输出任何信息。一般这个参数保持默认的0，这样可以帮我们更好的理解模型。\n",
    "学习目标参数：\n",
    "1． objective[默认是reg：linear]\n",
    "这个参数定义需要被最小化的损失函数。最常用的值有：binary：logistic二分类的逻辑回归，返回预测的概率非类别。multi:softmax使用softmax的多分类器，返回预测的类别。在这种情况下，你还要多设置一个参数：num_class类别数目。\n",
    "2． eval_metric[默认值取决于objective参数的取之]\n",
    "对于有效数据的度量方法。对于回归问题，默认值是rmse，对于分类问题，默认是error。典型值有：rmse均方根误差；mae平均绝对误差；logloss负对数似然函数值；error二分类错误率；merror多分类错误率；mlogloss多分类损失函数；auc曲线下面积。\n",
    "3． seed[默认是0]\n",
    "随机数的种子，设置它可以复现随机数据的结果，也可以用于调整参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本 = 4500\n",
      "验证样本 = 500\n",
      "测试样本 = 500\n",
      "训练样本特征表长度为 (4500, 12537)\n",
      "验证样本特征表长度为 (500, 12537)\n",
      "测试样本特征表长度为 (500, 12537)\n",
      "--------------------------------------\n",
      "XGBoost 开始训练了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/kazgu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.89267, std: 0.00599, params: {'reg_alpha': 0}, mean: 0.89311, std: 0.00603, params: {'reg_alpha': 0.001}, mean: 0.89244, std: 0.00586, params: {'reg_alpha': 0.005}, mean: 0.89289, std: 0.00628, params: {'reg_alpha': 0.01}, mean: 0.89222, std: 0.00657, params: {'reg_alpha': 0.05}]\n",
      "{'reg_alpha': 0.001}\n",
      "0.8931111111111111\n"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "data=pd.read_csv('data.csv',encoding=\"utf_8\",nrows=5000)# 少量数据集\n",
    "data=shuffle(data)\n",
    "d_train=shuffle(data.sample(frac=0.9))\n",
    "d_valid=shuffle(data.sample(frac=0.1))\n",
    "d_test=shuffle(data.sample(frac=0.1))\n",
    "data=readData(d_train ,d_valid, d_test)\n",
    "#svm_model(data)\n",
    "#RF_model(data)\n",
    "xgboost_model(data)\n",
    "scores1=scores\n",
    "###########################\n",
    "# scores=[]\n",
    "# data=pd.read_csv('data.csv',encoding=\"utf_8\")## 全数据\n",
    "# data=shuffle(data)\n",
    "# d_train=shuffle(data.sample(frac=0.9))\n",
    "# d_valid=shuffle(data.sample(frac=0.1))\n",
    "# d_test=shuffle(data.sample(frac=0.1))\n",
    "# data=readData(d_train ,d_valid, d_test)\n",
    "# svm_model(data)\n",
    "# RF_model(data)\n",
    "# xgboost_model(data)\n",
    "# DrawGraph(scores1)\n",
    "# DrawGraph(scores)\n",
    "# print('all finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过两次选择不同的大小的训练集，结果不太一致，数据集较大的时候SVM onevsrestclassifier 表现跟好的效果，randomForest几乎保持自己的状态\n",
    "，XGBoost稍微下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
